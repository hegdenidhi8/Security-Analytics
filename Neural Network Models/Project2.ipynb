{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name : Nidhi Hegde"
      ],
      "metadata": {
        "id": "I6yU85k6XQcX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USxZPM32W3Fu"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kK_X2WUW3Fw"
      },
      "source": [
        "## Dataset Description:\n",
        "The Drebin dataset comprises various Android applications, both benign and malicious. The features from these apps are extracted based on different aspects like:\n",
        "\n",
        "1. AndroidManifest.xml: Extracted details include requested permissions, app components like activities, services, etc.\n",
        "2. API calls: This includes specific Android API calls that the app makes.\n",
        "3. Network addresses: Any URLs or IP addresses that might be hardcoded in the app.\n",
        "4. Code patterns: Such as the use of reflection, native code, etc.\n",
        "\n",
        "The details of each feature is included in drebin_features.txt.\n",
        "\n",
        "The Drebin dataset primarily provides a binary label for each app, indicating whether it's benign or malicious. However, within the malicious apps, there can be different families of malware, each with specific characteristics and behaviors. While the main focus of the Drebin paper was on the binary classification task (malicious vs. benign), the authors did categorize the malicious samples into various malware families. These family labels can be used for multi-class classification tasks or for understanding the distribution of different types of malware in the dataset.\n",
        "\n",
        "Some malware families that might be present in such datasets (not limited to Drebin) include:\n",
        "\n",
        "**FakeInstaller:** Malware posing as a legitimate app installer.\n",
        "**DroidKungFu:** Known for exploiting several vulnerabilities and using encryption to hide its payloads.\n",
        "**Plankton:** Known for its stealthy nature and the ability to download and execute arbitrary code.\n",
        "**GingerMaster:** Exploits vulnerabilities specific to the Gingerbread version of Android.\n",
        "**BaseBridge:** Utilizes a privilege escalation exploit.\n",
        "... and others.\n",
        "\n",
        "The mapping between labels and malware families in our dataset is provided below:\n",
        "\n",
        "0: FakeInstaller\n",
        "\n",
        "1: DroidKungFu\n",
        "\n",
        "2: Plankton\n",
        "\n",
        "3: GingerMaster\n",
        "\n",
        "4: BaseBridge\n",
        "\n",
        "5: Iconosys\n",
        "\n",
        "6: Kmin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F4FtPwC9W3Fx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import other modules you may need"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej2904Cg4VwI",
        "outputId": "c238fad4-bd96-49f9-9541-7cd44198ed66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DUa8JK6iW3Fy"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "import numpy as np\n",
        "filepath = os.path.join('/content/drive/MyDrive/data/','drebin_data.npz')\n",
        "data = np.load(filepath)\n",
        "X, y = data['X'], data['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RrA56UtzW3Fz"
      },
      "outputs": [],
      "source": [
        "# split into training and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LUpSYTtYW3Fz"
      },
      "outputs": [],
      "source": [
        "# Design you MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            # define some middle layers\n",
        "            nn.ReLU(),  # Activation function for the first hidden layer\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),  # Activation function for the second hidden layer\n",
        "\n",
        "            nn.Linear(64, 7),\n",
        "            nn.Softmax(dim=1)  # Output layer with softmax activation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EfusEJOlW3Fz"
      },
      "outputs": [],
      "source": [
        "# Data Preparation(may convert them into tensors)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Uq-CTqqsW3Fz"
      },
      "outputs": [],
      "source": [
        "# Define your loss, optimizer, and other hyper-parameters\n",
        "batch_size = 67 #factor of 1340\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 1340 #number of features given in drebin_features.txt.\n",
        "model = MLP(input_size)\n",
        "criterion = nn.CrossEntropyLoss() #Loss function\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P6UNS2aEW3Fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8217ac47-9bca-4920-a04d-77f814dd376a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.3373757600784302, Test Loss: 1.3306015729904175, Test Acc: 0.8602825745682888\n",
            "Epoch 2/10, Loss: 1.2120193243026733, Test Loss: 1.1956560611724854, Test Acc: 0.9827315541601256\n",
            "Epoch 3/10, Loss: 1.1784517765045166, Test Loss: 1.1799172163009644, Test Acc: 0.9905808477237049\n",
            "Epoch 4/10, Loss: 1.1767877340316772, Test Loss: 1.1760783195495605, Test Acc: 0.9968602825745683\n",
            "Epoch 5/10, Loss: 1.1718500852584839, Test Loss: 1.1735628843307495, Test Acc: 0.9968602825745683\n",
            "Epoch 6/10, Loss: 1.1690107583999634, Test Loss: 1.1714403629302979, Test Acc: 0.9984301412872841\n",
            "Epoch 7/10, Loss: 1.1678078174591064, Test Loss: 1.1700644493103027, Test Acc: 0.9984301412872841\n",
            "Epoch 8/10, Loss: 1.1670539379119873, Test Loss: 1.1693918704986572, Test Acc: 0.9984301412872841\n",
            "Epoch 9/10, Loss: 1.16667902469635, Test Loss: 1.1692970991134644, Test Acc: 0.9984301412872841\n",
            "Epoch 10/10, Loss: 1.1663779020309448, Test Loss: 1.1689729690551758, Test Acc: 0.9984301412872841\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for i in range(0, X_train_tensor.shape[0], batch_size):\n",
        "        X_batch = X_train_tensor[i:i+batch_size]\n",
        "        y_batch = y_train_tensor[i:i+batch_size]\n",
        "        # Forward pass\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update the model's parameters\n",
        "\n",
        "    # Testing loss\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test_tensor)\n",
        "        test_loss = criterion(test_outputs, y_test_tensor)\n",
        "\n",
        "        _, predictions = torch.max(test_outputs, 1)  # Get the class with the highest probability\n",
        "        correct = (predictions == y_test_tensor).sum().item()\n",
        "        accuracy = correct / y_test_tensor.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Test Loss: {test_loss.item()}, Test Acc: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ij_nweTRW3F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a840eb8f-4064-4dd8-a495-c4e38dc4ec43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "FakeInstaller       0.99      1.00      1.00       177\n",
            "  DroidKungFu       1.00      0.99      1.00       136\n",
            "     Plankton       1.00      1.00      1.00       120\n",
            " GingerMaster       1.00      1.00      1.00        56\n",
            "   BaseBridge       1.00      1.00      1.00        76\n",
            "     Iconosys       1.00      1.00      1.00        40\n",
            "         Kmin       1.00      1.00      1.00        32\n",
            "\n",
            "     accuracy                           1.00       637\n",
            "    macro avg       1.00      1.00      1.00       637\n",
            " weighted avg       1.00      1.00      1.00       637\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate precision, recall, and F1-score for each class.\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Convert the model's predictions to a NumPy array\n",
        "y_pred = predictions.numpy()\n",
        "y_true = y_test_tensor.numpy()\n",
        "\n",
        "# Calculate the classification report\n",
        "class_names = ['FakeInstaller', 'DroidKungFu', 'Plankton', 'GingerMaster', 'BaseBridge', 'Iconosys', 'Kmin']\n",
        "report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "print(report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5kAFp98W3F0"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOuUJkUvW3F0"
      },
      "source": [
        "## Background:\n",
        "The paper \"Byteweight: Learning to recognize functions in binary code\" focuses on function boundary detection in binary code. One of the key insights of the paper is that specific byte sequences or n-grams are highly indicative of function starts. Detecting function boundaries is a foundational step for various binary analysis tasks such as disassembly, decompilation, and vulnerability discovery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ezZttmW3F1"
      },
      "source": [
        "## Dataset Description:\n",
        "The dataset derived from the Byteweight paper contains sequences of bytes extracted from binary files. These sequences represent potential function starts and other non-starting positions. Each byte in the sequence is treated as a token, and the goal is to recognize patterns that indicate the start of functions.\n",
        "\n",
        "Features: Sequences of bytes from binary files.\n",
        "Labels: Binary labels where '1' indicates the start of a function, and '0' indicates a non-starting position.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "yjcessNrW3F1"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "# import other modules you may need\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "TE3SbZrUW3F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feae2625-d33b-4013-efda-36995147ed15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: torch.Size([14006, 200])\n",
            "y_train shape: torch.Size([14006, 200])\n",
            "x_test shape: torch.Size([6003, 200])\n",
            "y_test shape: torch.Size([6003, 200])\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "train_file = os.path.join('/content/drive/MyDrive/data/','elf_x86_32_gcc_O1_train.pkl')\n",
        "test_file = os.path.join('/content/drive/MyDrive/data/','elf_x86_32_gcc_O1_test.pkl')\n",
        "\n",
        "# you may need to pre-process the data, for example, pad the data to some fixed length, for the RNN training\n",
        "# you can use pad_sequence in pytorch/keras, etc.\n",
        "\n",
        "# ...\n",
        "# Load training data\n",
        "with open(train_file, 'rb') as file:\n",
        "    train_data = pickle.load(file)\n",
        "\n",
        "# Load test data\n",
        "with open(test_file, 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "\n",
        "# Split the loaded data into input sequences (X) and labels (y)\n",
        "x_train, y_train = train_data\n",
        "x_test, y_test = test_data\n",
        "\n",
        "# you may need to pre-process the data, for example, pad the data to some fixed length, for the RNN training\n",
        "# you can use pad_sequence in pytorch/keras, etc.\n",
        "max_sequence_length = 200\n",
        "\n",
        "padded_x_train = [torch.LongTensor(sequence[:max_sequence_length]) for sequence in x_train]\n",
        "padded_y_train = [torch.LongTensor(sequence[:max_sequence_length]) for sequence in y_train]\n",
        "padded_x_test = [torch.LongTensor(sequence[:max_sequence_length]) for sequence in x_test]\n",
        "padded_y_test = [torch.LongTensor(sequence[:max_sequence_length]) for sequence in y_test]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = pad_sequence(padded_x_train, batch_first=True, padding_value=0)\n",
        "y_train_tensor = pad_sequence(padded_y_train, batch_first=True, padding_value=0)\n",
        "x_test_tensor = pad_sequence(padded_x_test, batch_first=True, padding_value=0)\n",
        "y_test_tensor = pad_sequence(padded_y_test, batch_first=True, padding_value=0)\n",
        "\n",
        "print(\"x_train shape:\", x_train_tensor.shape)\n",
        "print(\"y_train shape:\", y_train_tensor.shape)\n",
        "print(\"x_test shape:\", x_test_tensor.shape)\n",
        "print(\"y_test shape:\", y_test_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8SE8-_WyW3F1"
      },
      "outputs": [],
      "source": [
        "# Design you RNN model\n",
        "import numpy as np\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, seq_len, embedding_dim, hidden_dim, num_layers,num_classes, vocab_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # LSTM layer\n",
        "        out, _ = self.rnn(embedded)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "fQ1ZHunZW3F1"
      },
      "outputs": [],
      "source": [
        "# Define your loss, optimizer, and other hyperparameters\n",
        "batch_size = 124\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "input_size = 256\n",
        "hidden_dim = 47\n",
        "embedding_dim = 128\n",
        "num_layers = 1\n",
        "num_classes = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize your model\n",
        "seq_len = max_sequence_length\n",
        "model = RNNModel(seq_len, embedding_dim, hidden_dim, num_layers,num_classes, input_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "uKX1Qgg-W3F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05c261d4-5080-4628-9e81-e0aa8a8c59f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 9.854401923716068, Test Loss: 0.42064105393365026, Test Acc: 98.04827827477635%\n",
            "Epoch 2/10, Loss: 0.6157238709274679, Test Loss: 0.18306075234431773, Test Acc: 99.89242340946574%\n",
            "Epoch 3/10, Loss: 0.3438895173603669, Test Loss: 0.12440238625276834, Test Acc: 99.93635364086161%\n",
            "Epoch 4/10, Loss: 0.24791826965520158, Test Loss: 0.0948088314034976, Test Acc: 99.95579489229847%\n",
            "Epoch 5/10, Loss: 0.1945550343953073, Test Loss: 0.07700216543162242, Test Acc: 99.9657654055675%\n",
            "Epoch 6/10, Loss: 0.16050516767427325, Test Loss: 0.06464651474379934, Test Acc: 99.9723624369034%\n",
            "Epoch 7/10, Loss: 0.13744428643258289, Test Loss: 0.056210395821835846, Test Acc: 99.97528612124545%\n",
            "Epoch 8/10, Loss: 0.12068702638498507, Test Loss: 0.050151386531069875, Test Acc: 99.97683542405917%\n",
            "Epoch 9/10, Loss: 0.10878608193888795, Test Loss: 0.04574515858257655, Test Acc: 99.97923434454495%\n",
            "Epoch 10/10, Loss: 0.1001432819175534, Test Loss: 0.042297812542528845, Test Acc: 99.98088360237892%\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model in training mode\n",
        "    total_loss = 0.0\n",
        "    correct_total = 0\n",
        "    total = 0\n",
        "    for i in range(0, x_train_tensor.shape[0], batch_size):\n",
        "        batch_x = x_train_tensor[i:i+batch_size].to(device)\n",
        "        batch_y = y_train_tensor[i:i+batch_size].to(device).view(-1)\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(batch_x).view(-1,2)\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        # Backpropagate the loss and update the model's parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct = (outputs.argmax(dim=1) == batch_y).sum().item()\n",
        "        correct_total += correct\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    # Initialize test_loss here\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, x_test_tensor.shape[0], batch_size):\n",
        "            test_batch_x = x_test_tensor[i:i+batch_size].to(device)\n",
        "            test_batch_y = y_test_tensor[i:i+batch_size].to(device).view(-1)\n",
        "            test_outputs = model(test_batch_x).view(-1,2)\n",
        "            # Compute the test loss\n",
        "            test_loss += criterion(test_outputs, test_batch_y).item()\n",
        "            predicted_labels = torch.argmax(test_outputs, dim=1)\n",
        "            correct = (predicted_labels == test_batch_y).sum().item()\n",
        "            correct_total += correct\n",
        "            total += test_batch_y.size(0)\n",
        "\n",
        "    accuracy = 100 * correct_total / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}, Test Loss: {test_loss}, Test Acc: {accuracy}%\")\n",
        "# Save the trained model (if needed)\n",
        "#torch.save(model.state_dict(), 'model_file.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "mj26mQaJW3F2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f5dcc4-04a6-4b5e-ee2c-39572d20da10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.98%\n",
            "Precision: 95.91%\n",
            "Recall: 94.19%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the performance of your final model on test set using accuracy, precision and recall.\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Evaluate the performance on the test set\n",
        "model.eval()  # Set the model in evaluation mode\n",
        "\n",
        "# Lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, x_test_tensor.shape[0], batch_size):\n",
        "        test_batch_x = x_test_tensor[i:i+batch_size].to(device)\n",
        "        test_batch_y = y_test_tensor[i:i+batch_size].to(device).view(-1)\n",
        "        test_outputs = model(test_batch_x).view(-1, 2)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted = torch.argmax(test_outputs, dim=1)\n",
        "        true_labels.extend(test_batch_y.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy, precision, and recall\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}